{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the data pre-processing procedure, see S3 of the Appendix to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shutil import copy\n",
    "from scipy import interpolate\n",
    "from statsmodels.tsa.seasonal import STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_datasets(datalist, vdfname):\n",
    "    \"\"\" Creates Vensim script to convert CSVs to VDFs \"\"\"\n",
    "    print(\"Importing data to VDF...\")\n",
    "    scenario_text = []\n",
    "    scenario_text.append(\"SPECIAL>NOINTERACTION\\n\")\n",
    "    \n",
    "    for dataname in datalist:\n",
    "        scenario_text.append(f\"MENU>CSV2VDF|{dataname}.csv|{vdfname}{dataname}|{dataname}.frm|\\n\")\n",
    "    \n",
    "    scenario_text.append(\"MENU>EXIT\\n\")\n",
    "    \n",
    "    scriptfile = open(\"ImportData.cmd\", 'w')\n",
    "    scriptfile.writelines(scenario_text)\n",
    "    scriptfile.close()\n",
    "\n",
    "    \n",
    "def copy_data(datalist, vdfname):\n",
    "    \"\"\" Copies VDFXs to parent directory of working directory \"\"\"\n",
    "    for dataname in datalist:\n",
    "        for filetype in [\".vdf\", \".vdfx\"]:\n",
    "            try:\n",
    "                copy(f\"./{vdfname}{dataname}{filetype}\", f\"../\")\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "            \n",
    "def idx_to_int(df):\n",
    "    \"\"\"Converts string numeric column keys of dataframe to int\"\"\"\n",
    "    Tdf = df.T\n",
    "    Tdf.index = Tdf.index.astype('int')\n",
    "    newdf = Tdf.T\n",
    "    return(newdf)\n",
    "\n",
    "\n",
    "def get_first_idx(s):\n",
    "    return (s > 0).idxmax(skipna=True)\n",
    "\n",
    "\n",
    "def get_last_idx(s):\n",
    "    return s.where(s > 0).last_valid_index()\n",
    "\n",
    "\n",
    "def calculate_devs(flowrow, windowlength, datathreshold, thresholdwidth=1):\n",
    "    \"\"\"Calculate rolling mean of series and adjusted deviations from the mean, as well as \n",
    "    threshold values based on median +/- MADs, ignoring values below given datathreshold\"\"\"\n",
    "    flowmeanraw = flowrow.rolling(windowlength, min_periods=1, center=True).mean()\n",
    "    flowmean = flowmeanraw.copy()\n",
    "    flowmean.loc[:(flowmean >= datathreshold).idxmax()] = np.nan\n",
    "    flowrawdev = flowrow - flowmean\n",
    "    flowadjdev = flowrawdev / np.sqrt(flowmean)\n",
    "    lowthreshold = flowadjdev.median() - flowadjdev.mad() * thresholdwidth\n",
    "    highthreshold = flowadjdev.median() + flowadjdev.mad() * thresholdwidth\n",
    "    devs = {'rawmean': flowmeanraw, 'mean': flowmean, 'rawdev': flowrawdev, \n",
    "            'adjdev': flowadjdev, 'lowthr': lowthreshold, 'highthr': highthreshold}\n",
    "    return devs\n",
    "\n",
    "\n",
    "def fill_dips(smflow, smdevs, k, smoothfactor, lowthreshold, borrowlength=7):\n",
    "    \"\"\"Identify points with deviations below threshold value and partially fill \n",
    "    by borrowing from following points, based on a multinomial draw with probabilities \n",
    "    proportional to deviations of those points\"\"\"\n",
    "    for i, adjdev in enumerate(smdevs['adjdev'][:-k]):\n",
    "        if adjdev < lowthreshold:\n",
    "            borrowlist = smdevs['adjdev'].iloc[i+1:max(i+1+borrowlength, i+1)]\n",
    "            values = smflow.iloc[i+1:max(i+1+borrowlength, i+1)]\n",
    "            borrowlist -= adjdev\n",
    "            borrowlist.mask(borrowlist < 0, other=0, inplace=True)\n",
    "            if not all([(b == 0 or np.isnan(b)) for b in borrowlist]):\n",
    "                borrowlist.astype('float64')\n",
    "                borrowlist.dropna(inplace=True)\n",
    "                borrowlist /= borrowlist.sum()\n",
    "                mnlist = np.random.multinomial(abs(int(np.floor(smdevs['rawdev'].iloc[i]*smoothfactor))), \n",
    "                                               [abs(i) for i in borrowlist])\n",
    "                mnlist = np.minimum(mnlist, values)\n",
    "                smflow.iloc[i] += mnlist.sum()\n",
    "                for j, val in enumerate(mnlist):\n",
    "                    smflow.iloc[i+1+j] -= val\n",
    "\n",
    "                \n",
    "def smooth_peaks(smflow, smdevs, k, smoothfactor, highthreshold, distlength=14):\n",
    "    \"\"\"Identify points with deviations above threshold value and partially flatten \n",
    "    by distributing to preceding points, based on a multinomial draw with probabilities \n",
    "    proportional to existing rolling means of those points\"\"\"\n",
    "    for i, adjdev in reversed(list(enumerate(smdevs['adjdev'][:-k]))):\n",
    "        if adjdev > highthreshold:\n",
    "            distlist = smdevs['rawmean'].iloc[max(0, i-distlength):i]\n",
    "            if not all([(d == 0 or np.isnan(d)) for d in distlist]):\n",
    "                distlist.astype('float64')\n",
    "                distlist /= distlist.sum()\n",
    "                mnlist = np.random.multinomial(abs(int(np.floor(smdevs['rawdev'].iloc[i]*smoothfactor))), distlist)\n",
    "                smflow.iloc[i] -= mnlist.sum()\n",
    "                for j, val in enumerate(mnlist):\n",
    "                    smflow.iloc[i-len(mnlist)+j] += val\n",
    "\n",
    "\n",
    "def iter_smooth(smflow, ordevs, windowlength, datathreshold, smoothfactor, \n",
    "                borrowlength=7, distlength=14, iterlimit=10):\n",
    "    \"\"\"Iteratively apply dip-filling and peak-smoothing algorithms until \n",
    "    all deviations are within the upper and lower median+/-MAD thresholds\"\"\"\n",
    "    smdevs = calculate_devs(smflow, windowlength, datathreshold)\n",
    "    i = 0\n",
    "    while i < iterlimit:\n",
    "        # If mean values are too low, skip all smoothing\n",
    "        if np.nanmax(smdevs['mean']) < datathreshold:\n",
    "            break\n",
    "        # Identify last valid index and check if below threshold\n",
    "        k = smflow.index.get_loc(get_last_idx(smflow))\n",
    "        k = len(smflow) - k\n",
    "        # Identify all consecutive final terms below threshold to skip, otherwise will cause errors\n",
    "        while smdevs['adjdev'].iloc[-k] < ordevs['lowthr']:\n",
    "            k +=1\n",
    "        if np.nanmin(smdevs['adjdev'][:-k]) < ordevs['lowthr']:\n",
    "            fill_dips(smflow, smdevs, k, smoothfactor, ordevs['lowthr'])\n",
    "            smdevs = calculate_devs(smflow, windowlength, datathreshold)\n",
    "        if np.nanmax(smdevs['adjdev'][:-k]) > ordevs['highthr']:\n",
    "            smooth_peaks(smflow, smdevs, k, smoothfactor, ordevs['highthr'])\n",
    "            smdevs = calculate_devs(smflow, windowlength, datathreshold)\n",
    "        if (np.nanmax(smdevs['adjdev'][:-k]) < ordevs['highthr'] \n",
    "            and np.nanmin(smdevs['adjdev'][:-k]) > ordevs['lowthr']):\n",
    "            break\n",
    "        i += 1\n",
    "    return smflow\n",
    "\n",
    "\n",
    "def cross_corr(x, y, shift):\n",
    "    \"\"\"Get time-shifted cross-correlations of two series\"\"\"\n",
    "    if shift > 0:\n",
    "        xshift = x[0:-shift]\n",
    "        yshift = y[shift:]\n",
    "    elif shift < 0:\n",
    "        xshift = x[-shift:]\n",
    "        yshift = y[0:shift]\n",
    "    elif shift == 0:\n",
    "        xshift = x\n",
    "        yshift = y\n",
    "\n",
    "    rawcorrs = np.correlate(xshift, yshift, mode='full')\n",
    "    normcorr = rawcorrs[(rawcorrs.size // 2):] / np.amax(rawcorrs)\n",
    "    \n",
    "    return normcorr[0]\n",
    "\n",
    "\n",
    "def time_shift(x, shift):\n",
    "    \"\"\"Shift a series by a specified amount\"\"\"\n",
    "    xshift = x.copy()\n",
    "    if shift > 0:\n",
    "        xshift[shift:] = x[0:-shift]\n",
    "    elif shift < 0:\n",
    "        xshift[0:shift] = x[-shift:]\n",
    "    elif shift == 0:\n",
    "        pass\n",
    "    return xshift\n",
    "\n",
    "    \n",
    "def smooth_data(datalist, skiplist):\n",
    "    \"\"\"Run data smoothing and time shifting on data\"\"\"    \n",
    "    print(\"Executing smoothing algorithm!\")\n",
    "    \n",
    "    # Import dataframes from CSV and drop variable names\n",
    "    testdf = pd.read_csv(f\"{datalist['test']}.csv\", index_col=1,header=0)\n",
    "    testdf.drop(columns='Time', inplace=True)\n",
    "\n",
    "    formdf = pd.read_csv(f\"{datalist['form']}.csv\", index_col=1,header=0)\n",
    "    formdf.drop(columns='Time', inplace=True)\n",
    "\n",
    "    flowdf = pd.read_csv(f\"{datalist['flow']}.csv\",index_col=1,header=0)\n",
    "    flowdf.drop(columns='Time', inplace=True)\n",
    "\n",
    "    # Convert string indices to int\n",
    "    testdf = idx_to_int(testdf)\n",
    "    formdf = idx_to_int(formdf)\n",
    "    flowdf = idx_to_int(flowdf)\n",
    "\n",
    "    # Set up sub-dataframes from main data files\n",
    "    infdf = flowdf[0:nrows].copy()\n",
    "    dthdf = flowdf[nrows:(nrows*2)].copy()\n",
    "    recdf = flowdf[(nrows*2):(nrows*3)].copy()\n",
    "    tratedf = testdf.replace(testdf, np.nan)\n",
    "    tcapdf = testdf.replace(testdf, np.nan)\n",
    "    \n",
    "    # Convert infinite values to NaN to avoid potential errors\n",
    "    testdf.replace([np.inf, -np.inf], np.NaN)\n",
    "    \n",
    "    for i in testdf.index:\n",
    "        # Check if country is in skiplist\n",
    "        if i in skiplist:\n",
    "            print(f\"Repressing {i}!\")\n",
    "            continue\n",
    "        \n",
    "        # Check if country has sufficient test data to proceed, else skip\n",
    "        elif len(testdf.loc[i].dropna()) > mintestpoints:\n",
    "\n",
    "            # Ensure cumulative test data is strictly monotonic increasing\n",
    "            # NOTE: if monotonicity check happens after date value assignment, \n",
    "            # then if last test data point is nonmonotonic, it will be dropped causing an error\n",
    "            testdf.loc[i] = testdf.loc[i].mask(testdf.loc[i].cummax().duplicated())\n",
    "\n",
    "            # Identify first and last infection, test, and death date indices\n",
    "            infA, testA = [get_first_idx(s) for s in [infdf.loc[i], testdf.loc[i]]]\n",
    "            infZ, testZ, dthZ = [get_last_idx(s) for s in [infdf.loc[i], testdf.loc[i], dthdf.loc[i]]]\n",
    "\n",
    "            # Assign 0 test value to first infection date if before first test date\n",
    "            if infA < testA:\n",
    "                newtestA = infA\n",
    "                testdf.loc[i, newtestA] = 0\n",
    "            else:\n",
    "                newtestA = testA\n",
    "\n",
    "            # Set test rate and capacity values to 0 before first data point\n",
    "            tratedf.loc[i, :newtestA], tcapdf.loc[i, :newtestA] = 0, 0\n",
    "\n",
    "            # Check whether original test data is sparse in latter half of test data window\n",
    "            halftestrow = testdf.loc[i, newtestA:testZ]\n",
    "            halftestrow = halftestrow.iloc[len(halftestrow)//2:]\n",
    "            if len(halftestrow.dropna())/len(halftestrow) > 0.5:\n",
    "                smcheck = False\n",
    "            else:\n",
    "                smcheck = True\n",
    "                print(i, \"is sparse:\", len(testdf.loc[i]), len(halftestrow), len(halftestrow.dropna()))\n",
    "\n",
    "            # Interpolate test data using PCHIP spline if possible, within range of presumed test data\n",
    "            spline = interpolate.PchipInterpolator(testdf.loc[i].dropna().index, testdf.loc[i].dropna().values)\n",
    "            interptests = spline(testdf.loc[i, newtestA:testZ].index)\n",
    "\n",
    "            # Check if any interpolated values are negative; if so do linear interpolation instead\n",
    "            if any((interptests[1:] - interptests[:-1]) < 0):\n",
    "                print(\"Uh-oh, negative spline result, going linear!\")\n",
    "                linear = interpolate.interp1d(testdf.loc[i].dropna().index, testdf.loc[i].dropna().values)\n",
    "                interptests = linear(testdf.loc[i, newtestA:testZ].index)\n",
    "\n",
    "            # Assign interpolated values back to test data\n",
    "            testdf.loc[i, newtestA:testZ] = interptests\n",
    "            tratedf.loc[i, newtestA:testZ] = np.insert((interptests[1:] - interptests[:-1]), 0, interptests[0])\n",
    "\n",
    "            # If original test data is sparse, smooth test and infection data\n",
    "            if smcheck:\n",
    "                tratedevs = calculate_devs(tratedf.loc[i, newtestA:testZ], windowlength, datathreshold)\n",
    "                tratedf.loc[i, newtestA:testZ] = iter_smooth(tratedf.loc[i, newtestA:testZ], tratedevs, \n",
    "                                                             windowlength, datathreshold, smoothfactor)\n",
    "                infdevs = calculate_devs(infdf.loc[i, :infZ], windowlength, datathreshold)\n",
    "                infdf.loc[i, :infZ] = iter_smooth(infdf.loc[i, :infZ], infdevs, windowlength, datathreshold, smoothfactor)\n",
    "\n",
    "            # Else if original test data not sparse, do time shift on test data\n",
    "            else:\n",
    "                minlen = min(len(tratedf.loc[i].dropna()), len(infdf.loc[i].dropna()))\n",
    "                if minlen == 0:\n",
    "                    print(f\"Insufficient data for {i} shift, skipping!\")\n",
    "                else:\n",
    "                    x = STL(tratedf.loc[i].dropna(), period=7, seasonal=7).fit().seasonal\n",
    "                    y = STL(infdf.loc[i].dropna(), period=7, seasonal=7).fit().seasonal\n",
    "\n",
    "                    alseas = x.align(y, join='inner')\n",
    "\n",
    "                    seascorrs = []\n",
    "                    shiftrange = list(range(-2,5))\n",
    "\n",
    "                    for j in shiftrange:\n",
    "                        seascorrs.append(cross_corr(alseas[0], alseas[1], j))\n",
    "\n",
    "                    tshift = shiftrange[np.argmax(seascorrs)]\n",
    "                    shifttrate = time_shift(tratedf.loc[i], tshift)\n",
    "\n",
    "                    tratedf.loc[i] = shifttrate\n",
    "                    newtestA += tshift\n",
    "                    testZ += tshift\n",
    "\n",
    "                    print(f\"{i} shift is {tshift}\")\n",
    "\n",
    "            # Run polyfit on test rate data for later use to estimate test capacity\n",
    "            # Test capacity will be estimated as max of fitted test rate LATER on whole DF\n",
    "            pfit = np.polyfit(tratedf.loc[i, newtestA:testZ].index, tratedf.loc[i, newtestA:testZ].values, 10)\n",
    "            tcapdf.loc[i, newtestA:testZ] = np.polyval(pfit, tratedf.loc[i, newtestA:testZ].index)\n",
    "\n",
    "            # Run iterative dip/peak smoothing on death rates for all countries with enough deaths\n",
    "            if np.nanmax(dthdf.loc[i]) > datathreshold:\n",
    "                dthdevs = calculate_devs(dthdf.loc[i, :dthZ], windowlength, datathreshold)\n",
    "                dthdf.loc[i, :dthZ] = iter_smooth(dthdf.loc[i, :dthZ], dthdevs, windowlength, datathreshold, smoothfactor)\n",
    "                \n",
    "        else:\n",
    "            print(f\"Not enough test data for {i}, skipping!\")\n",
    "    \n",
    "    # Combine flow data streams into one dataframe\n",
    "    smflowdf = pd.concat([infdf, dthdf, recdf], axis=0)\n",
    "\n",
    "    # Set test capacity based on polyfit of test rate, ignoring first day\n",
    "    tcapdf.iloc[:, 1:] = tcapdf.iloc[:, 1:].cummax(axis=1, skipna=False)\n",
    "\n",
    "    # Recalculate cumulative tests based on smoothed test data\n",
    "    testdf = tratedf.cumsum(axis=1, skipna=False)\n",
    "\n",
    "    # Combine all three test data streams into one dataframe, dropping first day\n",
    "    smtestdf = pd.concat([testdf, tratedf, tcapdf], axis=0).iloc[:,1:]\n",
    "\n",
    "    # Shave NANs and last column of test dataframe\n",
    "    smtestdf.dropna(axis=1, how='all', inplace=True)\n",
    "    smtestdf = smtestdf.iloc[:,:-1]\n",
    "\n",
    "    # Adjust first day flows to account for non-zero initial cumulative values\n",
    "    smflowdf.iloc[:,0] += formdf.iloc[:,0]\n",
    "\n",
    "    # Recalculate cumulative data from smoothed flows, then readjust first day flows\n",
    "    smformdf = smflowdf.cumsum(axis=1)\n",
    "    smflowdf.iloc[:,0] -= formdf.iloc[:,0]\n",
    "\n",
    "    # Restore variable names and export to CSV\n",
    "    smflowdf.reset_index(inplace=True)\n",
    "    smflowdf.insert(0, 'Time', ['DataFlowInfection']*nrows+['DataFlowDeath']*nrows+['DataFlowRecovery']*nrows)\n",
    "    smflowdf.to_csv(f\"{datalist['flow']}.csv\", index=False)\n",
    "\n",
    "    smtestdf.reset_index(inplace=True)\n",
    "    smtestdf.insert(0, 'Time', ['DataCmltTest']*nrows+['DataTestRate']*nrows+['DataTestCapacity']*nrows)\n",
    "    smtestdf.to_csv(f\"{datalist['test']}.csv\", index=False)\n",
    "\n",
    "    smformdf.reset_index(inplace=True)\n",
    "    smformdf.insert(0, 'Time', ['DataCmltInfection']*nrows+['DataCmltDeath']*nrows+['DataCmltRecovery']*nrows)\n",
    "    smformdf.to_csv(f\"{datalist['form']}.csv\", index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlfilename = input(\"Enter control file name (with extension):\")\n",
    "controlfile = json.load(open(controlfilename, 'r'))\n",
    "\n",
    "# Unpack controlfile into variables\n",
    "for k,v in controlfile.items():\n",
    "    exec(k + '=v')\n",
    "\n",
    "if smoothing == True:\n",
    "    for k,v in smparams.items():\n",
    "        exec(k + '=v')\n",
    "    smooth_data(datalist, skiplist)\n",
    "\n",
    "import_datasets(datalist.values(), vdfname)\n",
    "\n",
    "subprocess.run(f\"{vensimpath} \\\"./ImportData.cmd\\\"\", check=True)\n",
    "\n",
    "copy_data(datalist.values(), vdfname)\n",
    "print(\"Job done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
